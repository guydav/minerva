{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "from datetime import datetime, date\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "import warnings\n",
    "import timeit\n",
    "from collections import defaultdict, OrderedDict\n",
    "import tabulate\n",
    "import time\n",
    "import GPy\n",
    "import glob\n",
    "from functools import reduce\n",
    "from IPython.display import display\n",
    "\n",
    "timeit.template = \"\"\"\n",
    "def inner(_it, _timer{init}):\n",
    "    {setup}\n",
    "    _t0 = _timer()\n",
    "    for _i in _it:\n",
    "        retval = {stmt}\n",
    "    _t1 = _timer()\n",
    "    return _t1 - _t0, retval\n",
    "\"\"\"\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "RANDOM_SEED = 33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Models\n",
    "The code below trains a simple Markov model for given data. The transition probabilities are simply the observed transition counts normalized to valid probability simplexes on each row of the matrix. \n",
    "\n",
    "In (somewhat) conforming with `sklearn` API, the model implements `fit` and `score`, as well as helper methods with convenience interfaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALPHABET = ['A', 'o', 'e', 't', 'p', 'g', 'k']\n",
    "TRAINING_FILE_PATTERN = 'data/markov/symbol/language-training-lang{lang}-*'\n",
    "TEST_FILE_PATTERN = 'data/markov/symbol/language-test-*'\n",
    "\n",
    "\n",
    "class MarkovModel:\n",
    "    def __init__(self, alphabet=ALPHABET):\n",
    "        self.map = {letter: index for (index, letter) in enumerate(ALPHABET)}\n",
    "        self.transitions = np.zeros((len(ALPHABET), len(ALPHABET)))\n",
    "        self.probabilities = np.ones_like(self.transitions)  # default initialization    \n",
    "    \n",
    "    def fit(self, sequence):\n",
    "        for first, second in zip(sequence, sequence[1:]):\n",
    "                self.transitions[self.map[first], self.map[second]] += 1\n",
    "                \n",
    "        self.probabilities = self.transitions / np.sum(self.transitions, axis=1)\n",
    "    \n",
    "    def fit_from_path(self, path):\n",
    "        with open(path) as f:\n",
    "            sequence = f.read()\n",
    "            self.fit(sequence)\n",
    "\n",
    "    def fit_from_glob(self, pattern):\n",
    "        for path in glob.glob(pattern):\n",
    "            self.fit_from_path(path)\n",
    "    \n",
    "    def score(self, sequence, log=False):\n",
    "        log_prob = np.sum([np.log(self.probabilities[self.map[first], self.map[second]])\n",
    "                       for first, second in zip(sequence, sequence[1:])])\n",
    "        if not log:\n",
    "            return np.exp(log_prob)\n",
    "\n",
    "        return log_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genertive Markov Model classifier\n",
    "\n",
    "The code below implements a simple generative Markov model classifier. For a new input sequence $v_{1:T}^*$, and a given class $c$, the model considers the posterior $P(c | v_{1:T}^*) \\propto P(v_{1:T}^* | c) P(c)$, the product of the data's likelihood according to the class multiplied by the prior for that class - Bayes's rule. To normalize, we must divide by the sum of posterior probabilities over all classes: \n",
    "\n",
    "$$ P(c | v_{1:T}^*) = \\frac{P(v_{1:T}^* | c) P(c)}{\\sum_{c' \\in C} P(v_{1:T}^* | c') P(c')} $$\n",
    "\n",
    "To classify, the model selects the argmax class over all class probabilities. In conforming with the `sklearn` API, the model implements `predict_proba`, `predict`, and `score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LANGUAGES = ('A', 'B', 'C')\n",
    "\n",
    "\n",
    "class GenerativeMarkovModelClassifier:\n",
    "    def __init__(self, prior=None, langauges=LANGUAGES):\n",
    "        self.n = len(langauges)\n",
    "        self.classes = langauges\n",
    "        self.models = [MarkovModel() for i in range(self.n)]\n",
    "        \n",
    "        if not prior:\n",
    "            self.prior = np.ones((self.n,)) / self.n\n",
    "\n",
    "        elif not type(prior) == dict:\n",
    "            if not len(prior) == self.n:\n",
    "                raise ValueError('Prior and languages should have the same length')\n",
    "                \n",
    "            if not 1.0 == sum(prior):\n",
    "                raise ValueError('Prior should sum up to 1')\n",
    "            \n",
    "            if any(prior < 0) or any (prior > 1):\n",
    "                raise ValueError('Prior should be a valid probability simplex')\n",
    "                \n",
    "            self.prior = prior\n",
    "    \n",
    "    def fit_from_pattern(self, training_file_pattern=TRAINING_FILE_PATTERN):\n",
    "        for (lang, model) in zip(self.classes, self.models):\n",
    "            model.fit_from_glob(training_file_pattern.format(lang=lang))\n",
    "\n",
    "    def predict_proba(self, sequence):\n",
    "        probs = np.array([model.score(sequence) for model in self.models]) * self.prior\n",
    "        return probs / np.sum(probs)\n",
    "            \n",
    "    def predict(self, sequence):\n",
    "        probs = self.predict_proba(sequence)\n",
    "        return self.classes[np.argmax(probs)]\n",
    "    \n",
    "    def score(self, sequence):\n",
    "        return np.max(self.predict_proba(sequence))\n",
    "    \n",
    "    def predict_from_pattern(self, test_file_pattern=TEST_FILE_PATTERN, probs=False):\n",
    "        predictions = []\n",
    "        \n",
    "        for path in glob.glob(test_file_pattern):\n",
    "            with open(path) as f:\n",
    "                sequence = f.read()\n",
    "                if probs:\n",
    "                    predictions.append(self.predict_proba(sequence))\n",
    "                else:\n",
    "                    predictions.append(self.predict(sequence))\n",
    "            \n",
    "        return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /Users/guydavidson/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:27: RuntimeWarning:divide by zero encountered in log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C', 'C', 'A', 'B', 'A', 'A', 'B', 'A', 'C', 'A']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmmc = GenerativeMarkovModelClassifier()\n",
    "gmmc.fit_from_pattern()\n",
    "gmmc.predict_from_pattern()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HMM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there are three speakers in the room, we should set $H = 3$, making $A$ a $3 \\times 3$ matrix, and given the same seven phonemes from the previous exercise, we should set $V = 7$, making $B$ a $3 \\times 7$ matrix.\n",
    "\n",
    "A reasonable assumption is to set $\\vec{a} = [\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}]^T$, a uniform distribution over the initial probabilities of each speaker starting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "\n",
    "$A_{ij} = P(h_{t + 1} = j | h_t = i)$, the row-major matrix notation.\n",
    "\n",
    "$B_{ij} = P(v_t = j | h_t = i)$, again, row-major, unlike Barber's notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open('data/hmm/speaker').read()\n",
    "transitions = list(zip(data, data[1:]))\n",
    "mapping = {letter: index for (index, letter) in enumerate(ALPHABET)}\n",
    "mapped_transitions = [(mapping[a], mapping[b]) for (a, b) in transitions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = 3\n",
    "V = 7\n",
    "A = np.array([[0.5, 0.25, 0.25], [0.25, 0.5, 0.25], [0.25, 0.25, 0.5]])\n",
    "# B = np.ones((H, V)) / V\n",
    "B = np.random.dirichlet(np.ones((V,)), 3)\n",
    "a = np.ones((H, 1)) / H    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def m_step(a, A, B, data):\n",
    "    mapped_data = [mapping[d] for d in data]\n",
    "    mapped_transitions = list(zip(mapped_data, mapped_data[1:]))\n",
    "    # to update a, we look at how likely the observed chain is to have started at i\n",
    "    a_new = np.zeros_like(a)\n",
    "    \n",
    "    for h_i in range(H):\n",
    "        log_p_h_i = 0\n",
    "        h = np.zeros_like(a)\n",
    "        h[h_i] = 1\n",
    "        for t in range(len(mapped_data)):\n",
    "            v_t = np.zeros((V, 1))\n",
    "            v_t[mapped_data[t]] = 1\n",
    "            p_v_t = np.matmul(B, v_t)\n",
    "            log_p_h_i += np.log(np.dot(h.T, p_v_t))\n",
    "            \n",
    "        a_new[h_i] = log_p_h_i\n",
    "      \n",
    "    a_new = a_new - a_new.max()\n",
    "    a_new = np.exp(a_new)\n",
    "    a_new = a_new / a_new.sum()\n",
    "    \n",
    "    # to update A, we look at the probabilities of the transitions, I guess\n",
    "    A_new = np.zeros_like(A)\n",
    "    for v_t, v_t_1 in mapped_transitions:\n",
    "        p_h_t = B[:,v_t].reshape((1, H))\n",
    "        p_h_t_1 = B[:,v_t_1].reshape((H, 1))\n",
    "        A_new += p_h_t * p_h_t_1\n",
    "        \n",
    "    A_new = A_new / A_new.sum(axis=1)\n",
    "    \n",
    "    # to update B, we do something similary?\n",
    "    B_new = np.zeros_like(B)\n",
    "    h = a.copy()\n",
    "    for t in range(len(mapped_data)):\n",
    "        v_t = np.zeros((V, 1))\n",
    "        v_t[mapped_data[t]] = 1\n",
    "        B_new +=  np.matmul(h, v_t.T)\n",
    "        h = np.matmul(A, h)\n",
    "\n",
    "    B_new = B_new / B_new.sum(axis=1).reshape((H, 1))\n",
    "    \n",
    "\n",
    "    return a_new, A_new, B_new\n",
    "  \n",
    "    \n",
    "a_new, A_new, B_new = m_step(a, A, B, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.176,  0.188,  0.141,  0.168,  0.167,  0.121,  0.039],\n",
       "       [ 0.176,  0.188,  0.141,  0.168,  0.167,  0.121,  0.039],\n",
       "       [ 0.176,  0.188,  0.141,  0.168,  0.167,  0.121,  0.039]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.33333333,  0.33333333,  0.33333333],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = np.zeros((V, 1))\n",
    "v[4] = 1\n",
    "\n",
    "p = np.ones((1, H)) / H\n",
    "\n",
    "\n",
    "np.matmul(v, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.14285714,  0.14285714,  0.14285714,  0.14285714,  0.14285714,\n",
       "         0.14285714,  0.14285714],\n",
       "       [ 0.14285714,  0.14285714,  0.14285714,  0.14285714,  0.14285714,\n",
       "         0.14285714,  0.14285714],\n",
       "       [ 0.14285714,  0.14285714,  0.14285714,  0.14285714,  0.14285714,\n",
       "         0.14285714,  0.14285714]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
